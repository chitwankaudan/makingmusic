{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Generation using an LSTM\n",
    "\n",
    "#### Final Project for Deep Learning (CS 7643)\n",
    "\n",
    "By Daeil Cha, Daniel Dias, Chitwan Kaudan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"../../lmd_matched\"\n",
    "data_path = \"../clean-data\"\n",
    "saved_models_path = \"../saved-models\"\n",
    "\n",
    "num_epochs = 10 # 1000\n",
    "batch_size = 8\n",
    "num_time_steps = 128\n",
    "num_total_songs = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from IPython.core.debugger import set_trace\n",
    "from getdata import getBatch\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pytorch GPU/CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# dtype = torch.FloatTensor\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "dtype = torch.cuda.FloatTensor\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "all_data = []\n",
    "\n",
    "while start < num_total_songs:\n",
    "    batch = np.array(getBatch(start, batch_size, num_time_steps, data_path), dtype='double')\n",
    "    all_data.append(batch)\n",
    "    # Shape should be (batch_size x num_time_steps x note_range x pitch/articulation)\n",
    "    start += batch_size\n",
    "\n",
    "all_data = np.concatenate(all_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 128, 78, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Input Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 78, 128, 80])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.input_function import InputKernel\n",
    "inputkernel = InputKernel.apply\n",
    "\n",
    "note_state_batch = torch.from_numpy(np.swapaxes(all_data,1,2)).float() \n",
    "#input kernel expects input shape = batch_size x num_notes x num_timesteps x 2\n",
    "midi_high = 101\n",
    "midi_low = 24\n",
    "time_init=0\n",
    "\n",
    "with torch.no_grad():\n",
    "    note_state_batch = inputkernel(note_state_batch,midi_low,midi_high,time_init)\n",
    "\n",
    "note_state_batch.shape\n",
    "#input kernel's output shape = batch_size x num_notes x num_timesteps x 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all data: (400, 128, 156)\n",
      "all expected: (400, 128, 156)\n"
     ]
    }
   ],
   "source": [
    "x_train = None\n",
    "y_train = None\n",
    "\n",
    "x_val = None\n",
    "y_val = None\n",
    "\n",
    "x_test = None\n",
    "y_test = None\n",
    "\n",
    "all_expected = np.empty(all_data.shape)\n",
    "all_expected[:, 0:all_expected.shape[1]-1] = all_data[:, 1:all_data.shape[1]]\n",
    "all_expected[:, all_expected.shape[1]-1] = 0\n",
    "\n",
    "all_data = np.reshape(all_data, (num_total_songs, num_time_steps, -1))\n",
    "all_expected = np.reshape(all_expected, (num_total_songs, num_time_steps, -1))\n",
    "\n",
    "print(\"all data:\", all_data.shape)\n",
    "print(\"all expected:\", all_expected.shape)\n",
    "\n",
    "note_state_batch.requires_grad_()\n",
    "\n",
    "orig_dataset = torch.utils.data.TensorDataset(note_state_batch.type(torch.FloatTensor), torch.from_numpy(all_expected).type(torch.FloatTensor))\n",
    "x_train, x_test = torch.utils.data.random_split(orig_dataset, [num_total_songs - 50, 50])\n",
    "\n",
    "x_train_loader = torch.utils.data.DataLoader(x_train, batch_size=batch_size, shuffle=True)\n",
    "x_test_loader = torch.utils.data.DataLoader(x_test, batch_size=batch_size)\n",
    "\n",
    "# for data in x_train_loader:\n",
    "#     print(data[0].shape, data[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y, model, loss_criterion, optimizer):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    # loss = loss_criterion(torch.max(y_pred, dim=1).indices, y)\n",
    "    loss = loss_criterion(y_pred, y)\n",
    "    ret_val = loss.item()\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return ret_val\n",
    "\n",
    "def test_step(x, y, model, loss_criterion):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_criterion(y_pred, y)\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save/Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name):\n",
    "    torch.save(model.state_dict(), os.path.join(saved_models_path, model_name))\n",
    "\n",
    "def load_model_parameters(model, model_name):\n",
    "    model.load_state_dict(torch.load(os.path.join(saved_models_path, model_name)))\n",
    "\n",
    "def load_new_model(model_name, model_constructor, *args):\n",
    "    model = model_constructor(args)\n",
    "    load_model_parameters(model, model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.main_model import MusicGenerationV2\n",
    "\n",
    "model = MusicGenerationV2(time_sequence_len=num_time_steps, batch_size=batch_size, time_hidden_size=36, data_type=dtype, device=device)\n",
    "\n",
    "load_model_parameters(model, \"biaxial_trained.pt\")\n",
    "\n",
    "loss_criterion = torch.nn.MSELoss(reduction='sum') # = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move To Correct Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MusicGenerationV2(\n",
       "  (lstm_time0): ModuleList(\n",
       "    (0): LSTM(80, 36, batch_first=True)\n",
       "    (1): LSTM(80, 36, batch_first=True)\n",
       "    (2): LSTM(80, 36, batch_first=True)\n",
       "    (3): LSTM(80, 36, batch_first=True)\n",
       "    (4): LSTM(80, 36, batch_first=True)\n",
       "    (5): LSTM(80, 36, batch_first=True)\n",
       "    (6): LSTM(80, 36, batch_first=True)\n",
       "    (7): LSTM(80, 36, batch_first=True)\n",
       "    (8): LSTM(80, 36, batch_first=True)\n",
       "    (9): LSTM(80, 36, batch_first=True)\n",
       "    (10): LSTM(80, 36, batch_first=True)\n",
       "    (11): LSTM(80, 36, batch_first=True)\n",
       "    (12): LSTM(80, 36, batch_first=True)\n",
       "    (13): LSTM(80, 36, batch_first=True)\n",
       "    (14): LSTM(80, 36, batch_first=True)\n",
       "    (15): LSTM(80, 36, batch_first=True)\n",
       "    (16): LSTM(80, 36, batch_first=True)\n",
       "    (17): LSTM(80, 36, batch_first=True)\n",
       "    (18): LSTM(80, 36, batch_first=True)\n",
       "    (19): LSTM(80, 36, batch_first=True)\n",
       "    (20): LSTM(80, 36, batch_first=True)\n",
       "    (21): LSTM(80, 36, batch_first=True)\n",
       "    (22): LSTM(80, 36, batch_first=True)\n",
       "    (23): LSTM(80, 36, batch_first=True)\n",
       "    (24): LSTM(80, 36, batch_first=True)\n",
       "    (25): LSTM(80, 36, batch_first=True)\n",
       "    (26): LSTM(80, 36, batch_first=True)\n",
       "    (27): LSTM(80, 36, batch_first=True)\n",
       "    (28): LSTM(80, 36, batch_first=True)\n",
       "    (29): LSTM(80, 36, batch_first=True)\n",
       "    (30): LSTM(80, 36, batch_first=True)\n",
       "    (31): LSTM(80, 36, batch_first=True)\n",
       "    (32): LSTM(80, 36, batch_first=True)\n",
       "    (33): LSTM(80, 36, batch_first=True)\n",
       "    (34): LSTM(80, 36, batch_first=True)\n",
       "    (35): LSTM(80, 36, batch_first=True)\n",
       "    (36): LSTM(80, 36, batch_first=True)\n",
       "    (37): LSTM(80, 36, batch_first=True)\n",
       "    (38): LSTM(80, 36, batch_first=True)\n",
       "    (39): LSTM(80, 36, batch_first=True)\n",
       "    (40): LSTM(80, 36, batch_first=True)\n",
       "    (41): LSTM(80, 36, batch_first=True)\n",
       "    (42): LSTM(80, 36, batch_first=True)\n",
       "    (43): LSTM(80, 36, batch_first=True)\n",
       "    (44): LSTM(80, 36, batch_first=True)\n",
       "    (45): LSTM(80, 36, batch_first=True)\n",
       "    (46): LSTM(80, 36, batch_first=True)\n",
       "    (47): LSTM(80, 36, batch_first=True)\n",
       "    (48): LSTM(80, 36, batch_first=True)\n",
       "    (49): LSTM(80, 36, batch_first=True)\n",
       "    (50): LSTM(80, 36, batch_first=True)\n",
       "    (51): LSTM(80, 36, batch_first=True)\n",
       "    (52): LSTM(80, 36, batch_first=True)\n",
       "    (53): LSTM(80, 36, batch_first=True)\n",
       "    (54): LSTM(80, 36, batch_first=True)\n",
       "    (55): LSTM(80, 36, batch_first=True)\n",
       "    (56): LSTM(80, 36, batch_first=True)\n",
       "    (57): LSTM(80, 36, batch_first=True)\n",
       "    (58): LSTM(80, 36, batch_first=True)\n",
       "    (59): LSTM(80, 36, batch_first=True)\n",
       "    (60): LSTM(80, 36, batch_first=True)\n",
       "    (61): LSTM(80, 36, batch_first=True)\n",
       "    (62): LSTM(80, 36, batch_first=True)\n",
       "    (63): LSTM(80, 36, batch_first=True)\n",
       "    (64): LSTM(80, 36, batch_first=True)\n",
       "    (65): LSTM(80, 36, batch_first=True)\n",
       "    (66): LSTM(80, 36, batch_first=True)\n",
       "    (67): LSTM(80, 36, batch_first=True)\n",
       "    (68): LSTM(80, 36, batch_first=True)\n",
       "    (69): LSTM(80, 36, batch_first=True)\n",
       "    (70): LSTM(80, 36, batch_first=True)\n",
       "    (71): LSTM(80, 36, batch_first=True)\n",
       "    (72): LSTM(80, 36, batch_first=True)\n",
       "    (73): LSTM(80, 36, batch_first=True)\n",
       "    (74): LSTM(80, 36, batch_first=True)\n",
       "    (75): LSTM(80, 36, batch_first=True)\n",
       "    (76): LSTM(80, 36, batch_first=True)\n",
       "    (77): LSTM(80, 36, batch_first=True)\n",
       "  )\n",
       "  (lstm_note0): ModuleList(\n",
       "    (0): LSTM(36, 2, batch_first=True)\n",
       "    (1): LSTM(36, 2, batch_first=True)\n",
       "    (2): LSTM(36, 2, batch_first=True)\n",
       "    (3): LSTM(36, 2, batch_first=True)\n",
       "    (4): LSTM(36, 2, batch_first=True)\n",
       "    (5): LSTM(36, 2, batch_first=True)\n",
       "    (6): LSTM(36, 2, batch_first=True)\n",
       "    (7): LSTM(36, 2, batch_first=True)\n",
       "    (8): LSTM(36, 2, batch_first=True)\n",
       "    (9): LSTM(36, 2, batch_first=True)\n",
       "    (10): LSTM(36, 2, batch_first=True)\n",
       "    (11): LSTM(36, 2, batch_first=True)\n",
       "    (12): LSTM(36, 2, batch_first=True)\n",
       "    (13): LSTM(36, 2, batch_first=True)\n",
       "    (14): LSTM(36, 2, batch_first=True)\n",
       "    (15): LSTM(36, 2, batch_first=True)\n",
       "    (16): LSTM(36, 2, batch_first=True)\n",
       "    (17): LSTM(36, 2, batch_first=True)\n",
       "    (18): LSTM(36, 2, batch_first=True)\n",
       "    (19): LSTM(36, 2, batch_first=True)\n",
       "    (20): LSTM(36, 2, batch_first=True)\n",
       "    (21): LSTM(36, 2, batch_first=True)\n",
       "    (22): LSTM(36, 2, batch_first=True)\n",
       "    (23): LSTM(36, 2, batch_first=True)\n",
       "    (24): LSTM(36, 2, batch_first=True)\n",
       "    (25): LSTM(36, 2, batch_first=True)\n",
       "    (26): LSTM(36, 2, batch_first=True)\n",
       "    (27): LSTM(36, 2, batch_first=True)\n",
       "    (28): LSTM(36, 2, batch_first=True)\n",
       "    (29): LSTM(36, 2, batch_first=True)\n",
       "    (30): LSTM(36, 2, batch_first=True)\n",
       "    (31): LSTM(36, 2, batch_first=True)\n",
       "    (32): LSTM(36, 2, batch_first=True)\n",
       "    (33): LSTM(36, 2, batch_first=True)\n",
       "    (34): LSTM(36, 2, batch_first=True)\n",
       "    (35): LSTM(36, 2, batch_first=True)\n",
       "    (36): LSTM(36, 2, batch_first=True)\n",
       "    (37): LSTM(36, 2, batch_first=True)\n",
       "    (38): LSTM(36, 2, batch_first=True)\n",
       "    (39): LSTM(36, 2, batch_first=True)\n",
       "    (40): LSTM(36, 2, batch_first=True)\n",
       "    (41): LSTM(36, 2, batch_first=True)\n",
       "    (42): LSTM(36, 2, batch_first=True)\n",
       "    (43): LSTM(36, 2, batch_first=True)\n",
       "    (44): LSTM(36, 2, batch_first=True)\n",
       "    (45): LSTM(36, 2, batch_first=True)\n",
       "    (46): LSTM(36, 2, batch_first=True)\n",
       "    (47): LSTM(36, 2, batch_first=True)\n",
       "    (48): LSTM(36, 2, batch_first=True)\n",
       "    (49): LSTM(36, 2, batch_first=True)\n",
       "    (50): LSTM(36, 2, batch_first=True)\n",
       "    (51): LSTM(36, 2, batch_first=True)\n",
       "    (52): LSTM(36, 2, batch_first=True)\n",
       "    (53): LSTM(36, 2, batch_first=True)\n",
       "    (54): LSTM(36, 2, batch_first=True)\n",
       "    (55): LSTM(36, 2, batch_first=True)\n",
       "    (56): LSTM(36, 2, batch_first=True)\n",
       "    (57): LSTM(36, 2, batch_first=True)\n",
       "    (58): LSTM(36, 2, batch_first=True)\n",
       "    (59): LSTM(36, 2, batch_first=True)\n",
       "    (60): LSTM(36, 2, batch_first=True)\n",
       "    (61): LSTM(36, 2, batch_first=True)\n",
       "    (62): LSTM(36, 2, batch_first=True)\n",
       "    (63): LSTM(36, 2, batch_first=True)\n",
       "    (64): LSTM(36, 2, batch_first=True)\n",
       "    (65): LSTM(36, 2, batch_first=True)\n",
       "    (66): LSTM(36, 2, batch_first=True)\n",
       "    (67): LSTM(36, 2, batch_first=True)\n",
       "    (68): LSTM(36, 2, batch_first=True)\n",
       "    (69): LSTM(36, 2, batch_first=True)\n",
       "    (70): LSTM(36, 2, batch_first=True)\n",
       "    (71): LSTM(36, 2, batch_first=True)\n",
       "    (72): LSTM(36, 2, batch_first=True)\n",
       "    (73): LSTM(36, 2, batch_first=True)\n",
       "    (74): LSTM(36, 2, batch_first=True)\n",
       "    (75): LSTM(36, 2, batch_first=True)\n",
       "    (76): LSTM(36, 2, batch_first=True)\n",
       "    (77): LSTM(36, 2, batch_first=True)\n",
       "    (78): LSTM(36, 2, batch_first=True)\n",
       "    (79): LSTM(36, 2, batch_first=True)\n",
       "    (80): LSTM(36, 2, batch_first=True)\n",
       "    (81): LSTM(36, 2, batch_first=True)\n",
       "    (82): LSTM(36, 2, batch_first=True)\n",
       "    (83): LSTM(36, 2, batch_first=True)\n",
       "    (84): LSTM(36, 2, batch_first=True)\n",
       "    (85): LSTM(36, 2, batch_first=True)\n",
       "    (86): LSTM(36, 2, batch_first=True)\n",
       "    (87): LSTM(36, 2, batch_first=True)\n",
       "    (88): LSTM(36, 2, batch_first=True)\n",
       "    (89): LSTM(36, 2, batch_first=True)\n",
       "    (90): LSTM(36, 2, batch_first=True)\n",
       "    (91): LSTM(36, 2, batch_first=True)\n",
       "    (92): LSTM(36, 2, batch_first=True)\n",
       "    (93): LSTM(36, 2, batch_first=True)\n",
       "    (94): LSTM(36, 2, batch_first=True)\n",
       "    (95): LSTM(36, 2, batch_first=True)\n",
       "    (96): LSTM(36, 2, batch_first=True)\n",
       "    (97): LSTM(36, 2, batch_first=True)\n",
       "    (98): LSTM(36, 2, batch_first=True)\n",
       "    (99): LSTM(36, 2, batch_first=True)\n",
       "    (100): LSTM(36, 2, batch_first=True)\n",
       "    (101): LSTM(36, 2, batch_first=True)\n",
       "    (102): LSTM(36, 2, batch_first=True)\n",
       "    (103): LSTM(36, 2, batch_first=True)\n",
       "    (104): LSTM(36, 2, batch_first=True)\n",
       "    (105): LSTM(36, 2, batch_first=True)\n",
       "    (106): LSTM(36, 2, batch_first=True)\n",
       "    (107): LSTM(36, 2, batch_first=True)\n",
       "    (108): LSTM(36, 2, batch_first=True)\n",
       "    (109): LSTM(36, 2, batch_first=True)\n",
       "    (110): LSTM(36, 2, batch_first=True)\n",
       "    (111): LSTM(36, 2, batch_first=True)\n",
       "    (112): LSTM(36, 2, batch_first=True)\n",
       "    (113): LSTM(36, 2, batch_first=True)\n",
       "    (114): LSTM(36, 2, batch_first=True)\n",
       "    (115): LSTM(36, 2, batch_first=True)\n",
       "    (116): LSTM(36, 2, batch_first=True)\n",
       "    (117): LSTM(36, 2, batch_first=True)\n",
       "    (118): LSTM(36, 2, batch_first=True)\n",
       "    (119): LSTM(36, 2, batch_first=True)\n",
       "    (120): LSTM(36, 2, batch_first=True)\n",
       "    (121): LSTM(36, 2, batch_first=True)\n",
       "    (122): LSTM(36, 2, batch_first=True)\n",
       "    (123): LSTM(36, 2, batch_first=True)\n",
       "    (124): LSTM(36, 2, batch_first=True)\n",
       "    (125): LSTM(36, 2, batch_first=True)\n",
       "    (126): LSTM(36, 2, batch_first=True)\n",
       "    (127): LSTM(36, 2, batch_first=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      " - 440 : 16070.888671875\n",
      " --- saving model to biaxial_440.pt ---\n",
      " - 441 : 15140.724609375\n",
      " - 442 : 14763.494140625\n",
      " - 443 : 15935.17578125\n",
      " - 444 : 14846.9150390625\n",
      " - 445 : 16343.25\n",
      " - 446 : 15737.119140625\n",
      " - 447 : 16946.623046875\n",
      " - 448 : 15418.9873046875\n",
      " - 449 : 15382.314453125\n",
      " - 450 : 18391.083984375\n",
      " - 451 : 15160.7939453125\n",
      " - 452 : 13694.53515625\n",
      " - 453 : 15783.974609375\n",
      " - 454 : 16505.181640625\n",
      " - 455 : 14993.923828125\n",
      " - 456 : 15638.017578125\n",
      " - 457 : 14972.087890625\n",
      " - 458 : 16685.34765625\n",
      " - 459 : 15875.21484375\n",
      " - 460 : 16582.951171875\n",
      " - 461 : 16996.46875\n",
      " - 462 : 14915.5537109375\n",
      " - 463 : 16376.2734375\n",
      " - 464 : 19302.96875\n",
      " - 465 : 15234.9189453125\n",
      " - 466 : 14397.5595703125\n",
      " - 467 : 16689.197265625\n",
      " - 468 : 18130.07421875\n",
      " - 469 : 17242.421875\n",
      " - 470 : 17544.97265625\n",
      " - 471 : 16837.28515625\n",
      " - 472 : 15778.6259765625\n",
      " - 473 : 16975.931640625\n",
      " - 474 : 16205.9453125\n",
      " - 475 : 16643.896484375\n",
      " - 476 : 17150.14453125\n",
      " - 477 : 17185.294921875\n",
      " - 478 : 16674.583984375\n",
      " - 479 : 16219.31640625\n",
      " - 480 : 15910.763671875\n",
      " --- saving model to biaxial_480.pt ---\n",
      " - 481 : 15856.06640625\n",
      " - 482 : 15231.927734375\n",
      " - 483 : 12096.7919921875\n",
      "Epoch: 1\n",
      " - 484 : 16956.60546875\n",
      " - 485 : 14337.75390625\n",
      " - 486 : 16156.7255859375\n",
      " - 487 : 16669.8828125\n",
      " - 488 : 16348.03515625\n",
      " - 489 : 17003.357421875\n",
      " - 490 : 16164.91796875\n",
      " - 491 : 15261.5556640625\n",
      " - 492 : 17029.35546875\n",
      " - 493 : 15746.0498046875\n",
      " - 494 : 16393.951171875\n",
      " - 495 : 16031.703125\n",
      " - 496 : 16695.912109375\n",
      " - 497 : 15276.7021484375\n",
      " - 498 : 15183.3759765625\n",
      " - 499 : 15685.455078125\n",
      " - 500 : 16339.13671875\n",
      " - 501 : 14697.080078125\n",
      " - 502 : 16739.482421875\n",
      " - 503 : 15746.9072265625\n",
      " - 504 : 17022.94921875\n",
      " - 505 : 15671.328125\n",
      " - 506 : 15478.12890625\n",
      " - 507 : 16049.4921875\n",
      " - 508 : 17594.865234375\n",
      " - 509 : 16432.77734375\n",
      " - 510 : 16957.353515625\n",
      " - 511 : 16285.0517578125\n",
      " - 512 : 17055.376953125\n",
      " - 513 : 16220.0166015625\n",
      " - 514 : 15338.763671875\n",
      " - 515 : 16356.3310546875\n",
      " - 516 : 13580.484375\n",
      " - 517 : 16432.451171875\n",
      " - 518 : 15367.357421875\n",
      " - 519 : 16559.181640625\n",
      " - 520 : 16212.7431640625\n",
      " - 521 : 15434.01953125\n",
      " - 522 : 17269.87109375\n",
      " - 523 : 16267.044921875\n",
      " - 524 : 18586.435546875\n",
      " - 525 : 15419.076171875\n",
      " - 526 : 15374.9375\n",
      " - 527 : 12523.978515625\n",
      "Epoch: 2\n",
      " - 528 : 16397.421875\n",
      " - 529 : 16465.1328125\n",
      " - 530 : 15404.05078125\n",
      " --- saving model to biaxial_530.pt ---\n",
      " - 531 : 17358.810546875\n",
      " - 532 : 15646.265625\n",
      " - 533 : 16497.654296875\n",
      " - 534 : 15677.91015625\n",
      " - 535 : 15530.19140625\n",
      " - 536 : 15785.5400390625\n",
      " - 537 : 18099.54296875\n",
      " - 538 : 16614.2734375\n",
      " - 539 : 17411.16796875\n",
      " - 540 : 14878.8076171875\n",
      " --- saving model to biaxial_540.pt ---\n",
      " - 541 : 13899.2138671875\n",
      " - 542 : 15969.0361328125\n",
      " - 543 : 16668.939453125\n",
      " - 544 : 14969.28125\n",
      " - 545 : 16540.02734375\n",
      " - 546 : 14198.234375\n",
      " - 547 : 15493.896484375\n",
      " - 548 : 17558.2265625\n",
      " - 549 : 16219.779296875\n",
      " - 550 : 15982.1083984375\n",
      " - 551 : 15399.49609375\n",
      " - 552 : 17113.427734375\n",
      " - 553 : 15260.451171875\n",
      " - 554 : 14388.517578125\n",
      " - 555 : 16010.404296875\n",
      " - 556 : 15405.8466796875\n",
      " - 557 : 17533.107421875\n",
      " - 558 : 17598.248046875\n",
      " - 559 : 15862.365234375\n",
      " - 560 : 17322.755859375\n",
      " - 561 : 16611.40625\n",
      " - 562 : 15790.189453125\n",
      " - 563 : 14898.40625\n",
      " - 564 : 14596.28125\n",
      " - 565 : 17076.400390625\n",
      " - 566 : 15669.669921875\n",
      " - 567 : 18325.53515625\n",
      " - 568 : 16557.96875\n",
      " - 569 : 16724.693359375\n",
      " - 570 : 15198.798828125\n",
      " - 571 : 12834.4580078125\n",
      "Epoch: 3\n",
      " - 572 : 17414.408203125\n",
      " - 573 : 15196.861328125\n",
      " - 574 : 15434.283203125\n",
      " - 575 : 18386.44921875\n",
      " - 576 : 15005.439453125\n",
      " - 577 : 16459.361328125\n",
      " - 578 : 15919.333984375\n",
      " - 579 : 15980.658203125\n",
      " - 580 : 16237.341796875\n",
      " - 581 : 15674.900390625\n",
      " - 582 : 15014.056640625\n",
      " - 583 : 14890.7900390625\n",
      " - 584 : 15813.8271484375\n",
      " - 585 : 16269.900390625\n",
      " - 586 : 15830.369140625\n",
      " - 587 : 14953.501953125\n",
      " - 588 : 15906.1259765625\n",
      " - 589 : 15441.484375\n",
      " - 590 : 15816.033203125\n",
      " - 591 : 15357.673828125\n",
      " - 592 : 17116.86328125\n",
      " - 593 : 15789.9228515625\n",
      " - 594 : 17740.828125\n",
      " - 595 : 16674.47265625\n",
      " - 596 : 16709.998046875\n",
      " - 597 : 15885.27734375\n",
      " - 598 : 15999.4658203125\n",
      " - 599 : 16429.88671875\n",
      " - 600 : 16791.4453125\n",
      " - 601 : 16100.5625\n",
      " - 602 : 15639.5107421875\n",
      " - 603 : 15442.421875\n",
      " - 604 : 16563.623046875\n",
      " - 605 : 15939.24609375\n",
      " - 606 : 16964.740234375\n",
      " - 607 : 16155.677734375\n",
      " - 608 : 16118.279296875\n",
      " - 609 : 15935.3662109375\n",
      " - 610 : 15934.6787109375\n",
      " - 611 : 16703.38671875\n",
      " - 612 : 15942.88671875\n",
      " - 613 : 15979.9296875\n",
      " - 614 : 14755.25390625\n",
      " - 615 : 14618.751953125\n",
      "Epoch: 4\n",
      " - 616 : 14915.9970703125\n",
      " - 617 : 15967.1083984375\n",
      " - 618 : 14274.48046875\n",
      " - 619 : 16991.29296875\n",
      " - 620 : 16716.72265625\n",
      " - 621 : 15824.40234375\n",
      " - 622 : 15623.6845703125\n",
      " - 623 : 16342.25390625\n",
      " - 624 : 16935.64453125\n",
      " - 625 : 14787.845703125\n",
      " - 626 : 15741.26171875\n",
      " - 627 : 15279.076171875\n",
      " - 628 : 16508.74609375\n",
      " - 629 : 16837.58984375\n",
      " - 630 : 15686.177734375\n",
      " - 631 : 15413.6875\n",
      " - 632 : 16090.0869140625\n",
      " - 633 : 14777.181640625\n",
      " - 634 : 15975.384765625\n",
      " - 635 : 15038.6435546875\n",
      " - 636 : 16110.7705078125\n",
      " - 637 : 16722.041015625\n",
      " - 638 : 15879.2021484375\n",
      " - 639 : 17907.77734375\n",
      " - 640 : 17941.67578125\n",
      " - 641 : 15929.2890625\n",
      " - 642 : 16246.705078125\n",
      " - 643 : 14282.05078125\n",
      " - 644 : 13746.203125\n",
      " - 645 : 17633.20703125\n",
      " - 646 : 16654.09765625\n",
      " - 647 : 15516.3505859375\n",
      " - 648 : 16118.3125\n",
      " - 649 : 16312.2353515625\n",
      " - 650 : 17777.26171875\n",
      " - 651 : 15613.1640625\n",
      " - 652 : 17787.287109375\n",
      " - 653 : 15846.513671875\n",
      " - 654 : 16297.86328125\n",
      " - 655 : 16425.30078125\n",
      " - 656 : 16463.7734375\n",
      " - 657 : 17089.140625\n",
      " - 658 : 16951.81640625\n",
      " - 659 : 11448.5205078125\n",
      "Epoch: 5\n",
      " - 660 : 16001.736328125\n",
      " - 661 : 15403.755859375\n",
      " - 662 : 14568.2958984375\n",
      " - 663 : 16523.908203125\n",
      " - 664 : 16355.7900390625\n",
      " - 665 : 15085.1962890625\n",
      " - 666 : 17137.8203125\n",
      " - 667 : 16209.03515625\n",
      " - 668 : 16694.580078125\n",
      " - 669 : 16415.578125\n",
      " - 670 : 14575.001953125\n",
      " --- saving model to biaxial_670.pt ---\n",
      " - 671 : 16653.701171875\n",
      " - 672 : 15849.6748046875\n",
      " - 673 : 14552.66796875\n",
      " - 674 : 16532.228515625\n",
      " - 675 : 14375.982421875\n",
      " - 676 : 16265.947265625\n",
      " - 677 : 16069.796875\n",
      " - 678 : 17457.044921875\n",
      " - 679 : 16594.181640625\n",
      " - 680 : 16680.818359375\n",
      " - 681 : 14968.4013671875\n",
      " - 682 : 16055.3984375\n",
      " - 683 : 19292.658203125\n",
      " - 684 : 16768.8203125\n",
      " - 685 : 18095.5390625\n",
      " - 686 : 18019.04296875\n",
      " - 687 : 17410.578125\n",
      " - 688 : 18197.751953125\n",
      " - 689 : 15699.541015625\n",
      " - 690 : 16086.361328125\n",
      " - 691 : 16291.037109375\n",
      " - 692 : 15643.408203125\n",
      " - 693 : 15318.119140625\n",
      " - 694 : 15634.5078125\n",
      " - 695 : 14148.671875\n",
      " - 696 : 16071.9814453125\n",
      " - 697 : 13446.38671875\n",
      " - 698 : 16577.734375\n",
      " - 699 : 14540.4892578125\n",
      " - 700 : 15969.70703125\n",
      " - 701 : 15375.814453125\n",
      " - 702 : 15903.3701171875\n",
      " - 703 : 12402.1162109375\n",
      "Epoch: 6\n",
      " - 704 : 16946.35546875\n",
      " - 705 : 16216.955078125\n",
      " - 706 : 16649.7421875\n",
      " - 707 : 15822.505859375\n",
      " - 708 : 14966.275390625\n",
      " - 709 : 17144.87109375\n",
      " - 710 : 16762.60546875\n",
      " - 711 : 15523.7685546875\n",
      " - 712 : 15761.5546875\n",
      " - 713 : 17347.68359375\n",
      " - 714 : 15843.890625\n",
      " - 715 : 15684.330078125\n",
      " - 716 : 17292.42578125\n",
      " - 717 : 15039.1181640625\n",
      " - 718 : 15385.951171875\n",
      " - 719 : 16112.365234375\n",
      " - 720 : 17079.720703125\n",
      " - 721 : 16477.10546875\n",
      " - 722 : 16043.193359375\n",
      " - 723 : 16298.9267578125\n",
      " - 724 : 16081.9150390625\n",
      " - 725 : 14574.1162109375\n",
      " - 726 : 14726.0234375\n",
      " - 727 : 15427.9853515625\n",
      " - 728 : 14482.453125\n",
      " - 729 : 16056.17578125\n",
      " - 730 : 16204.9697265625\n",
      " - 731 : 15624.046875\n",
      " - 732 : 16576.62109375\n",
      " - 733 : 18912.75390625\n",
      " - 734 : 16263.95703125\n",
      " - 735 : 17186.830078125\n",
      " - 736 : 16156.3017578125\n",
      " - 737 : 17383.76953125\n",
      " - 738 : 15819.1298828125\n",
      " - 739 : 16579.1171875\n",
      " - 740 : 14908.8115234375\n",
      " - 741 : 15444.5234375\n",
      " - 742 : 15428.06640625\n",
      " - 743 : 16916.486328125\n",
      " - 744 : 15898.4482421875\n",
      " - 745 : 16020.8896484375\n",
      " - 746 : 14122.3427734375\n",
      " - 747 : 12216.4560546875\n",
      "Epoch: 7\n",
      " - 748 : 16860.171875\n",
      " - 749 : 14397.115234375\n",
      " - 750 : 16765.6953125\n",
      " - 751 : 14209.6982421875\n",
      " - 752 : 16902.505859375\n",
      " - 753 : 15360.12109375\n",
      " - 754 : 15993.3076171875\n",
      " - 755 : 15442.634765625\n",
      " - 756 : 15984.5830078125\n",
      " - 757 : 16122.0390625\n",
      " - 758 : 15830.61328125\n",
      " - 759 : 16148.779296875\n",
      " - 760 : 15661.16015625\n",
      " - 761 : 16524.51953125\n",
      " - 762 : 15709.77734375\n",
      " - 763 : 15096.5078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 764 : 16196.279296875\n",
      " - 765 : 16377.634765625\n",
      " - 766 : 15572.0703125\n",
      " - 767 : 16580.439453125\n",
      " - 768 : 15807.7529296875\n",
      " - 769 : 14741.787109375\n",
      " - 770 : 17865.5234375\n",
      " - 771 : 16867.08984375\n",
      " - 772 : 17999.84765625\n",
      " - 773 : 16488.08984375\n",
      " - 774 : 14913.998046875\n",
      " - 775 : 15485.26953125\n",
      " - 776 : 16326.689453125\n",
      " - 777 : 16964.328125\n",
      " - 778 : 15296.5322265625\n",
      " - 779 : 15727.779296875\n",
      " - 780 : 15637.296875\n",
      " - 781 : 17760.251953125\n",
      " - 782 : 17247.1875\n",
      " - 783 : 16832.326171875\n",
      " - 784 : 14751.974609375\n",
      " - 785 : 17255.2421875\n",
      " - 786 : 15664.984375\n",
      " - 787 : 15933.0986328125\n",
      " - 788 : 14636.2744140625\n",
      " - 789 : 16551.82421875\n",
      " - 790 : 16325.73828125\n",
      " - 791 : 12085.1455078125\n",
      "Epoch: 8\n",
      " - 792 : 15543.9619140625\n",
      " - 793 : 17061.05859375\n",
      " - 794 : 15741.96875\n",
      " - 795 : 14579.19921875\n",
      " - 796 : 18780.974609375\n",
      " - 797 : 16560.892578125\n",
      " - 798 : 14844.484375\n",
      " - 799 : 16929.109375\n",
      " - 800 : 16556.00390625\n",
      " - 801 : 16046.263671875\n",
      " - 802 : 15461.892578125\n",
      " - 803 : 16804.48828125\n",
      " - 804 : 15967.0576171875\n",
      " - 805 : 16780.51953125\n",
      " - 806 : 17053.80078125\n",
      " - 807 : 15305.177734375\n",
      " - 808 : 16117.623046875\n",
      " - 809 : 14367.896484375\n",
      " - 810 : 16338.3623046875\n",
      " - 811 : 15933.734375\n",
      " - 812 : 16373.572265625\n",
      " - 813 : 14940.5244140625\n",
      " - 814 : 15784.7939453125\n",
      " - 815 : 15086.2197265625\n",
      " - 816 : 15852.951171875\n",
      " - 817 : 15762.361328125\n",
      " - 818 : 16021.759765625\n",
      " - 819 : 16647.02734375\n",
      " - 820 : 16003.978515625\n",
      " - 821 : 16135.712890625\n",
      " - 822 : 15415.44921875\n",
      " - 823 : 17041.69140625\n",
      " - 824 : 15677.916015625\n",
      " - 825 : 16400.56640625\n",
      " - 826 : 14671.3125\n",
      " - 827 : 16534.10546875\n",
      " - 828 : 17266.859375\n",
      " - 829 : 16255.912109375\n",
      " - 830 : 15812.5556640625\n",
      " - 831 : 15679.03515625\n",
      " - 832 : 16961.076171875\n",
      " - 833 : 15694.4638671875\n",
      " - 834 : 15379.9716796875\n",
      " - 835 : 12217.19921875\n",
      "Epoch: 9\n",
      " - 836 : 16836.369140625\n",
      " - 837 : 16606.07421875\n",
      " - 838 : 17096.544921875\n",
      " - 839 : 14803.7783203125\n",
      " - 840 : 15472.4111328125\n",
      " - 841 : 15046.580078125\n",
      " - 842 : 15121.2275390625\n",
      " - 843 : 15680.240234375\n",
      " - 844 : 15927.08203125\n",
      " - 845 : 17232.876953125\n",
      " - 846 : 16828.537109375\n",
      " - 847 : 14568.173828125\n",
      " - 848 : 16133.89453125\n",
      " - 849 : 15557.83984375\n",
      " - 850 : 14486.55859375\n",
      " --- saving model to biaxial_850.pt ---\n",
      " - 851 : 16059.4169921875\n",
      " - 852 : 15663.5517578125\n",
      " - 853 : 15323.158203125\n",
      " - 854 : 17190.728515625\n",
      " - 855 : 15853.2509765625\n",
      " - 856 : 15706.458984375\n",
      " - 857 : 18030.884765625\n",
      " - 858 : 17751.9921875\n",
      " - 859 : 18103.986328125\n",
      " - 860 : 18124.958984375\n",
      " - 861 : 14725.083984375\n",
      " - 862 : 14501.228515625\n",
      " - 863 : 14822.439453125\n",
      " - 864 : 17004.48046875\n",
      " - 865 : 16706.4140625\n",
      " - 866 : 15326.470703125\n",
      " - 867 : 15688.3173828125\n",
      " - 868 : 17049.232421875\n",
      " - 869 : 16368.4482421875\n",
      " - 870 : 15546.9931640625\n",
      " - 871 : 15212.88671875\n",
      " - 872 : 15862.296875\n",
      " - 873 : 16021.484375\n",
      " - 874 : 16893.6875\n",
      " - 875 : 14772.4482421875\n",
      " - 876 : 15799.6923828125\n",
      " - 877 : 16698.55078125\n",
      " - 878 : 15796.15234375\n",
      " - 879 : 11879.392578125\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "best_loss = 30000\n",
    "\n",
    "# num_train_steps = 0\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    losses.append([])\n",
    "\n",
    "    for i, data in enumerate(x_train_loader, 0):\n",
    "        x, y = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        loss = train_step(x, y, model, loss_criterion, optimizer)\n",
    "        losses[epoch].append(loss)\n",
    "\n",
    "        print(\" -\", num_train_steps, \":\", loss)\n",
    "        if num_train_steps % 10 == 0 and loss < best_loss:\n",
    "            best_loss = loss\n",
    "            save_model(model, \"biaxial_{0}.pt\".format(num_train_steps))\n",
    "            print(\" --- saving model to biaxial_{0}.pt ---\".format(num_train_steps))\n",
    "        \n",
    "        num_train_steps += 1\n",
    "    losses[epoch] = np.asarray(losses[epoch])\n",
    "            \n",
    "save_model(model, \"biaxial_trained.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"biaxial_epochs30.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14941.841796875\n",
      "16315.681640625\n",
      "16645.833984375\n",
      "16636.525390625\n",
      "16352.1640625\n",
      "15169.2607421875\n",
      "3877.65380859375\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in x_test_loader:\n",
    "        x, y = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        loss = test_step(x, y, model, loss_criterion)\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IPDB Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_trace()\n",
    "\n",
    "def test_code():\n",
    "    set_trace()\n",
    "\n",
    "    train_iter = iter(x_train_loader)\n",
    "    data = next(train_iter)\n",
    "    x, y = data[0].to(device), data[1].to(device)    \n",
    "\n",
    "    loss = train_step(x, y, model, loss_criterion, optimizer)\n",
    "    \n",
    "    print(loss)\n",
    "    \n",
    "    data = next(train_iter)\n",
    "    x, y = data[0].to(device), data[1].to(device)\n",
    "\n",
    "    loss = train_step(x, y, model, loss_criterion, optimizer)\n",
    "    \n",
    "    print(loss)\n",
    "# test_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "[16070.88867188 15140.72460938 14763.49414062 15935.17578125\n",
      " 14846.91503906 16343.25       15737.11914062 16946.62304688\n",
      " 15418.98730469 15382.31445312 18391.08398438 15160.79394531\n",
      " 13694.53515625 15783.97460938 16505.18164062 14993.92382812\n",
      " 15638.01757812 14972.08789062 16685.34765625 15875.21484375\n",
      " 16582.95117188 16996.46875    14915.55371094 16376.2734375\n",
      " 19302.96875    15234.91894531 14397.55957031 16689.19726562\n",
      " 18130.07421875 17242.421875   17544.97265625 16837.28515625\n",
      " 15778.62597656 16975.93164062 16205.9453125  16643.89648438\n",
      " 17150.14453125 17185.29492188 16674.58398438 16219.31640625\n",
      " 15910.76367188 15856.06640625 15231.92773438 12096.79199219]\n",
      "[16956.60546875 14337.75390625 16156.72558594 16669.8828125\n",
      " 16348.03515625 17003.35742188 16164.91796875 15261.55566406\n",
      " 17029.35546875 15746.04980469 16393.95117188 16031.703125\n",
      " 16695.91210938 15276.70214844 15183.37597656 15685.45507812\n",
      " 16339.13671875 14697.08007812 16739.48242188 15746.90722656\n",
      " 17022.94921875 15671.328125   15478.12890625 16049.4921875\n",
      " 17594.86523438 16432.77734375 16957.35351562 16285.05175781\n",
      " 17055.37695312 16220.01660156 15338.76367188 16356.33105469\n",
      " 13580.484375   16432.45117188 15367.35742188 16559.18164062\n",
      " 16212.74316406 15434.01953125 17269.87109375 16267.04492188\n",
      " 18586.43554688 15419.07617188 15374.9375     12523.97851562]\n",
      "[16397.421875   16465.1328125  15404.05078125 17358.81054688\n",
      " 15646.265625   16497.65429688 15677.91015625 15530.19140625\n",
      " 15785.54003906 18099.54296875 16614.2734375  17411.16796875\n",
      " 14878.80761719 13899.21386719 15969.03613281 16668.93945312\n",
      " 14969.28125    16540.02734375 14198.234375   15493.89648438\n",
      " 17558.2265625  16219.77929688 15982.10839844 15399.49609375\n",
      " 17113.42773438 15260.45117188 14388.51757812 16010.40429688\n",
      " 15405.84667969 17533.10742188 17598.24804688 15862.36523438\n",
      " 17322.75585938 16611.40625    15790.18945312 14898.40625\n",
      " 14596.28125    17076.40039062 15669.66992188 18325.53515625\n",
      " 16557.96875    16724.69335938 15198.79882812 12834.45800781]\n",
      "[17414.40820312 15196.86132812 15434.28320312 18386.44921875\n",
      " 15005.43945312 16459.36132812 15919.33398438 15980.65820312\n",
      " 16237.34179688 15674.90039062 15014.05664062 14890.79003906\n",
      " 15813.82714844 16269.90039062 15830.36914062 14953.50195312\n",
      " 15906.12597656 15441.484375   15816.03320312 15357.67382812\n",
      " 17116.86328125 15789.92285156 17740.828125   16674.47265625\n",
      " 16709.99804688 15885.27734375 15999.46582031 16429.88671875\n",
      " 16791.4453125  16100.5625     15639.51074219 15442.421875\n",
      " 16563.62304688 15939.24609375 16964.74023438 16155.67773438\n",
      " 16118.27929688 15935.36621094 15934.67871094 16703.38671875\n",
      " 15942.88671875 15979.9296875  14755.25390625 14618.75195312]\n",
      "[14915.99707031 15967.10839844 14274.48046875 16991.29296875\n",
      " 16716.72265625 15824.40234375 15623.68457031 16342.25390625\n",
      " 16935.64453125 14787.84570312 15741.26171875 15279.07617188\n",
      " 16508.74609375 16837.58984375 15686.17773438 15413.6875\n",
      " 16090.08691406 14777.18164062 15975.38476562 15038.64355469\n",
      " 16110.77050781 16722.04101562 15879.20214844 17907.77734375\n",
      " 17941.67578125 15929.2890625  16246.70507812 14282.05078125\n",
      " 13746.203125   17633.20703125 16654.09765625 15516.35058594\n",
      " 16118.3125     16312.23535156 17777.26171875 15613.1640625\n",
      " 17787.28710938 15846.51367188 16297.86328125 16425.30078125\n",
      " 16463.7734375  17089.140625   16951.81640625 11448.52050781]\n",
      "[16001.73632812 15403.75585938 14568.29589844 16523.90820312\n",
      " 16355.79003906 15085.19628906 17137.8203125  16209.03515625\n",
      " 16694.58007812 16415.578125   14575.00195312 16653.70117188\n",
      " 15849.67480469 14552.66796875 16532.22851562 14375.98242188\n",
      " 16265.94726562 16069.796875   17457.04492188 16594.18164062\n",
      " 16680.81835938 14968.40136719 16055.3984375  19292.65820312\n",
      " 16768.8203125  18095.5390625  18019.04296875 17410.578125\n",
      " 18197.75195312 15699.54101562 16086.36132812 16291.03710938\n",
      " 15643.40820312 15318.11914062 15634.5078125  14148.671875\n",
      " 16071.98144531 13446.38671875 16577.734375   14540.48925781\n",
      " 15969.70703125 15375.81445312 15903.37011719 12402.11621094]\n",
      "[16946.35546875 16216.95507812 16649.7421875  15822.50585938\n",
      " 14966.27539062 17144.87109375 16762.60546875 15523.76855469\n",
      " 15761.5546875  17347.68359375 15843.890625   15684.33007812\n",
      " 17292.42578125 15039.11816406 15385.95117188 16112.36523438\n",
      " 17079.72070312 16477.10546875 16043.19335938 16298.92675781\n",
      " 16081.91503906 14574.11621094 14726.0234375  15427.98535156\n",
      " 14482.453125   16056.17578125 16204.96972656 15624.046875\n",
      " 16576.62109375 18912.75390625 16263.95703125 17186.83007812\n",
      " 16156.30175781 17383.76953125 15819.12988281 16579.1171875\n",
      " 14908.81152344 15444.5234375  15428.06640625 16916.48632812\n",
      " 15898.44824219 16020.88964844 14122.34277344 12216.45605469]\n",
      "[16860.171875   14397.11523438 16765.6953125  14209.69824219\n",
      " 16902.50585938 15360.12109375 15993.30761719 15442.63476562\n",
      " 15984.58300781 16122.0390625  15830.61328125 16148.77929688\n",
      " 15661.16015625 16524.51953125 15709.77734375 15096.5078125\n",
      " 16196.27929688 16377.63476562 15572.0703125  16580.43945312\n",
      " 15807.75292969 14741.78710938 17865.5234375  16867.08984375\n",
      " 17999.84765625 16488.08984375 14913.99804688 15485.26953125\n",
      " 16326.68945312 16964.328125   15296.53222656 15727.77929688\n",
      " 15637.296875   17760.25195312 17247.1875     16832.32617188\n",
      " 14751.97460938 17255.2421875  15664.984375   15933.09863281\n",
      " 14636.27441406 16551.82421875 16325.73828125 12085.14550781]\n",
      "[15543.96191406 17061.05859375 15741.96875    14579.19921875\n",
      " 18780.97460938 16560.89257812 14844.484375   16929.109375\n",
      " 16556.00390625 16046.26367188 15461.89257812 16804.48828125\n",
      " 15967.05761719 16780.51953125 17053.80078125 15305.17773438\n",
      " 16117.62304688 14367.89648438 16338.36230469 15933.734375\n",
      " 16373.57226562 14940.52441406 15784.79394531 15086.21972656\n",
      " 15852.95117188 15762.36132812 16021.75976562 16647.02734375\n",
      " 16003.97851562 16135.71289062 15415.44921875 17041.69140625\n",
      " 15677.91601562 16400.56640625 14671.3125     16534.10546875\n",
      " 17266.859375   16255.91210938 15812.55566406 15679.03515625\n",
      " 16961.07617188 15694.46386719 15379.97167969 12217.19921875]\n",
      "[16836.36914062 16606.07421875 17096.54492188 14803.77832031\n",
      " 15472.41113281 15046.58007812 15121.22753906 15680.24023438\n",
      " 15927.08203125 17232.87695312 16828.53710938 14568.17382812\n",
      " 16133.89453125 15557.83984375 14486.55859375 16059.41699219\n",
      " 15663.55175781 15323.15820312 17190.72851562 15853.25097656\n",
      " 15706.45898438 18030.88476562 17751.9921875  18103.98632812\n",
      " 18124.95898438 14725.08398438 14501.22851562 14822.43945312\n",
      " 17004.48046875 16706.4140625  15326.47070312 15688.31738281\n",
      " 17049.23242188 16368.44824219 15546.99316406 15212.88671875\n",
      " 15862.296875   16021.484375   16893.6875     14772.44824219\n",
      " 15799.69238281 16698.55078125 15796.15234375 11879.39257812]\n"
     ]
    }
   ],
   "source": [
    "print(len(loss))\n",
    "for loss in losses:\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs7643music] *",
   "language": "python",
   "name": "conda-env-cs7643music-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
